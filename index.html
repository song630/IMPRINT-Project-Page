<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="IMPRINT: Generative Object Compositing by Learning Identity-Preserving Representation">
  <!-- <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/> -->
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="image composition, image editing, personalization">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>IMPRINT: Generative Object Compositing by Learning Identity-Preserving Representation</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">IMPRINT: Generative Object Compositing by Learning Identity-Preserving Representation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Yizhi Song</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="https://zzutk.github.io/" target="_blank">Zhifei Zhang</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://sites.google.com/site/zhelin625/" target="_blank">Zhe Lin</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="https://research.adobe.com/person/scott-cohen/" target="_blank">Scott Cohen</a><sup>2</sup>,</span>
                      <span class="author-block">
                        <a href="https://www.brianpricephd.com/" target="_blank">Brian Price</a><sup>2</sup>,</span>
                        <span class="author-block">
                          <a href="https://jimmie33.github.io/" target="_blank">Jianming Zhang</a><sup>2</sup>,</span>
                          <span class="author-block">
                            <a href="https://sites.google.com/view/sooyekim" target="_blank">Soo Ye Kim</a><sup>2</sup>,</span>
                            <span class="author-block">
                              <a href="https://sites.google.com/site/hezhangsprinter/" target="_blank">He Zhang</a><sup>2</sup>,</span>
                              <span class="author-block">
                                <a href="https://wxiong.me/" target="_blank">Wei Xiong</a><sup>2</sup>,</span>
                                <span class="author-block">
                                  <a href="https://www.cs.purdue.edu/homes/aliaga/" target="_blank">Daniel Aliaga</a><sup>1</sup></span>
                    </span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- institution -->
                    <span class="author-block">Purdue University<sup>1</sup>, Adobe<sup>2</sup><br><p style="color: rgb(238, 165, 177);">In CVPR 2024</p></span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2403.10701.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Project (TBD)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2403.10701" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" width="150%" src="./static/images/teaser.png">
      <h2 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman">
          <b>
            Figure 1. Top: Comparison with three prior works, i.e., Paint-by-Example, ObjectStitch, and TF-ICON. Our method IMPRINT outperforms others in terms of identity preservation and color/geometry harmonization. Bottom: Given a coarse mask, IMPRINT can change the pose of the object to follow the shape of the mask.
          </b></p>
      </h2>
    </div>
  </div>
</section>
<!-- End teaser -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Generative object compositing emerges as a promising new avenue for compositional image editing. However, the requirement of object identity preservation poses a significant challenge, limiting practical usage of most existing methods. In response, this paper introduces IMPRINT, a novel diffusion-based generative model trained with a two-stage learning framework that decouples learning of identity preservation from that of compositing. The first stage is targeted for context-agnostic, identity-preserving pretraining of the object encoder, enabling the encoder to learn an embedding that is both view-invariant and conducive to enhanced detail preservation. The subsequent stage leverages this representation to learn seamless harmonization of the object composited to the background. In addition, IMPRINT incorporates a shape-guidance mechanism offering user-directed control over the compositing process. Extensive experiments demonstrate that IMPRINT significantly outperforms existing methods and various baselines on identity preservation and composition quality.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Pipeline -->
<section class="section"   style="background-color:white;">

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The Two-stage Pipeline</h2>
      </div>
    </div>
  </div>

  <div class="columns is-centered has-text-centered">
      <div style="width:90%;">

        <div style="float:left; width:47.1%; border: 0px solid black;">
          <h2 class="title is-5">Stage 1</h2>
          <div class="content has-text-justified">
            Stage of context-agnostic ID-preserving: we design a novel image encoder (with pre-trained DINOv2 as backbone) trained on multi-view object pairs to learn view-invariant ID-preserving representation.                
          </div>  

          <div class="column is-five-fifths">
              <div class="columns is-centered">
                <img id="modulated_training" width="105%" src="./static/images/stage1_pipeline.png">
              </div>
          </div>
        </div>

        <div style=" float:right; width:47.8%; border: 0px solid black;">
          <h2 class="title is-5">Stage 2</h2>
          <div class="content has-text-justified">
            Stage of object compositing: taking the learned image encoder from the first stage and freezing its backbone, the whole model is trained for compositing the object to the masked region (see Fig. 3 for the blending process).               
          </div>

          <div class="column is-five-fifths">
              <div class="columns is-centered">
                <img id="modulated_training" width="97%" src="./static/images/stage2_pipeline.png">
              </div>
          </div>
        </div>

      </div>  
    </div>
  
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          <p style="font-family:Times New Roman">
            <b>
              Figure 2. The two-stage training pipeline of the proposed IMPRINT.
            </b></p>
        </h2>
      </div>
    </div>
</section>
<!-- End of pipeline -->


<!-- Bg blending -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"> The Background-blending Process </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">       
            <img id="model" width="100%" src="./static/images/blending.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 3. Illustration of the background-blending process. At each denoising step, the background area of the denoised latent is masked and blended with unmasked area from the clean background (intuitively, the model is only denoising the foreground).</b></p>
            </h3>   

        </div>
  </div>
</section>
<!-- End of bg blending -->


<!-- Shape guidance -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"> Shape-guided Controllable Compositing </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">       
            <img id="model" width="100%" src="./static/images/mask_ctrl.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman">
                <b>Figure 4. More shape-control results. IMPRINT introduces more user control by using a user-provided mask as input. Inspired by SmartBrush, we define four types of mask (including bounding box). In addition to object compositing, our model also performs edits on the input object. Depending on the shape of the coarse mask, IMPRINT can operate different types of editing, including changing the view of an object, and applying non-rigid transformation on the object.
                </b></p>
            </h3>   

        </div>
  </div>
</section>
<!-- End of shape guidance -->


<!-- Ablation -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"> The ID-preserving Representation </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">       
            <img id="model" width="100%" src="./static/images/recon.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman">
                <b>Figure 5. Top: Results of context-agnostic ID-preserving pretraining (after the first stage); IMPRINT generates view pose changes while memorizing the details of the object. Bottom: Diverse poses of the object after the second stage.
                </b></p>
            </h3>   

        </div>
  </div>
</section>
<!-- End of ablation -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/qual1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison on the DreamBooth test set. Paint-by-Example and ObjectStitch lose most object details and only maintain categorical information. TF-ICON tends to copy the pose of the input subject. The comparison highlights the advantage of IMPRINT in keeping identity and making geometric changes.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/qual2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          More qualitative comparisons.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/qual3.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          More qualitative comparisons.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/qual4.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Additional qualitative comparisons with AnyDoor.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">

      <h2 class="title is-3">Video Presentation (coming soon)</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{song2024imprint,
    title={IMPRINT: Generative Object Compositing by Learning Identity-Preserving Representation},
    author={Song, Yizhi and Zhang, Zhifei and Lin, Zhe and Cohen, Scott and Price, Brian and Zhang, Jianming and Kim, Soo Ye and Zhang, He and Xiong, Wei and Aliaga, Daniel},
    journal={arXiv preprint arXiv:2403.10701},
    year={2024}
}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            </a>
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
